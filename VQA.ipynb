{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc2ffcc-9f31-4dca-a69d-27b1390992a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d416d7-cdf1-4610-bbd7-69bba083c503",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQAModel(nn.Module):\n",
    "    def __init__(self, num_classes=3000):\n",
    "        super(VQAModel, self).__init__()\n",
    "        \n",
    "        # Image encoder (using ResNet)\n",
    "        self.image_encoder = models.resnet50(pretrained=True)\n",
    "        self.image_encoder = nn.Sequential(*list(self.image_encoder.children())[:-1])\n",
    "        \n",
    "        # BERT for question encoding\n",
    "        self.question_encoder = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        # Fusion and classification layers\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(2048 + 768, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32a2909-e0c1-461d-b596-8add277e118c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, image, question_tokens):\n",
    "        # Image encoding\n",
    "        img_features = self.image_encoder(image)\n",
    "        img_features = img_features.view(img_features.size(0), -1)\n",
    "        \n",
    "        # Question encoding\n",
    "        question_features = self.question_encoder(**question_tokens)[1]\n",
    "        \n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat((img_features, question_features), dim=1)\n",
    "        \n",
    "        # Final classification\n",
    "        output = self.fusion(combined_features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8083b4c-4816-4e2a-ac63-041499059068",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQAPredictor:\n",
    "    def __init__(self, model_path=None, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.device = device\n",
    "        self.model = VQAModel().to(device)\n",
    "        if model_path:\n",
    "            self.model.load_state_dict(torch.load(model_path))\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Initialize tokenizer and image transforms\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.image_transforms = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        # Load answer vocabulary (simplified version)\n",
    "        self.idx2ans = {0: \"yes\", 1: \"no\", 2: \"2\", 3: \"1\", 4: \"3\", 5: \"4\",\n",
    "                       6: \"red\", 7: \"blue\", 8: \"green\", 9: \"white\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6547ac-1bea-48ae-bfa2-4c68eb2f88db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(self, image_path):\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = self.image_transforms(image).unsqueeze(0)\n",
    "        return image.to(self.device)\n",
    "    \n",
    "    def preprocess_question(self, question):\n",
    "        tokens = self.tokenizer(\n",
    "            question,\n",
    "            padding='max_length',\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {k: v.to(self.device) for k, v in tokens.items()}\n",
    "    \n",
    "    def predict(self, image_path, question):\n",
    "        \"\"\"\n",
    "        Make a prediction for a given image and question\n",
    "        \"\"\"\n",
    "        # Preprocess inputs\n",
    "        image = self.preprocess_image(image_path)\n",
    "        question_tokens = self.preprocess_question(question)\n",
    "        \n",
    "        # Get model prediction\n",
    "        with torch.no_grad():\n",
    "            output = self.model(image, question_tokens)\n",
    "            pred_idx = output.argmax(dim=1).item()\n",
    "        \n",
    "        # Convert prediction to answer\n",
    "        answer = self.idx2ans.get(pred_idx, \"I don't know\")\n",
    "        confidence = torch.softmax(output, dim=1)[0][pred_idx].item()\n",
    "        \n",
    "        return {\n",
    "            'answer': answer,\n",
    "            'confidence': confidence\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e909a0e4-4cd3-4e32-b31a-af9facb8b6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vqa_model(train_loader, val_loader, num_epochs=10):\n",
    "    \"\"\"\n",
    "    Training function for the VQA model\n",
    "    \"\"\"\n",
    "    model = VQAModel()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (images, questions, answers) in enumerate(train_loader):\n",
    "            images, questions, answers = (\n",
    "                images.to(device),\n",
    "                {k: v.to(device) for k, v in questions.items()},\n",
    "                answers.to(device)\n",
    "            )\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, questions)\n",
    "            loss = criterion(outputs, answers)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, questions, answers in val_loader:\n",
    "                images, questions, answers = (\n",
    "                    images.to(device),\n",
    "                    {k: v.to(device) for k, v in questions.items()},\n",
    "                    answers.to(device)\n",
    "                )\n",
    "                \n",
    "                outputs = model(images, questions)\n",
    "                loss = criterion(outputs, answers)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = outputs.max(1)\n",
    "                total += answers.size(0)\n",
    "                correct += predicted.eq(answers).sum().item()\n",
    "        \n",
    "        print(f'Epoch: {epoch}')\n",
    "        print(f'Training Loss: {train_loss/len(train_loader):.4f}')\n",
    "        print(f'Validation Loss: {val_loss/len(val_loader):.4f}')\n",
    "        print(f'Validation Accuracy: {100.*correct/total:.2f}%')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290372cd-f060-4f08-83bc-1b4ba8a3522b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize predictor\n",
    "    predictor = VQAPredictor()\n",
    "    \n",
    "    # Example prediction\n",
    "    result = predictor.predict(\n",
    "        image_path=\"example_image.jpg\",\n",
    "        question=\"What color is the car?\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
